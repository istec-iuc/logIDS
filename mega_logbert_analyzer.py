import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer
from sklearn.cluster import MiniBatchKMeans
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.decomposition import IncrementalPCA
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import gc
import os
import time
from pathlib import Path
import pickle
import json
warnings.filterwarnings('ignore')

class MegaDataLogBERT:
    """Ã‡ok bÃ¼yÃ¼k HDFS trace verisi iÃ§in optimize edilmiÅŸ LogBERT"""
    
    def __init__(self, model_name='bert-base-uncased', max_length=64, batch_size=8):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ”§ KullanÄ±lan cihaz: {self.device}")
        
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertModel.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        self.max_length = max_length
        self.batch_size = batch_size
        self.scaler = StandardScaler()
        
        # BÃ¼yÃ¼k veri parametreleri
        self.sample_size = 50000  # Ä°ÅŸlenecek maksimum log sayÄ±sÄ±
        self.chunk_size = 5000    # Her seferinde iÅŸlenecek chunk
        
    def load_trace_data(self, data_dir="data/HDFS_big.log"):
        """Trace veri setini yÃ¼kle ve analiz et"""
        print("ğŸ“Š MEGA TRACE VERÄ° SETÄ° ANALÄ°ZÄ° BAÅLADI")
        print("=" * 60)
        
        data_path = Path(data_dir)
        preprocessed_path = data_path / "preprocessed"
        tracebench_path = data_path / "tracebench"
        
        # Preprocessed verileri analiz et
        print("ğŸ” Preprocessed veriler analiz ediliyor...")
        
        normal_file = preprocessed_path / "normal_trace.csv"
        failure_file = preprocessed_path / "failure_trace.csv"
        
        normal_data = None
        failure_data = None
        
        try:
            # Normal trace verilerini yÃ¼kle (sample)
            print("ğŸ“– Normal trace veriler yÃ¼kleniyor...")
            normal_data = pd.read_csv(normal_file, nrows=self.sample_size//2)
            print(f"âœ… Normal trace: {len(normal_data):,} kayÄ±t yÃ¼klendi")
            
            # Failure trace verilerini yÃ¼kle (sample) 
            print("ğŸ“– Failure trace veriler yÃ¼kleniyor...")
            failure_data = pd.read_csv(failure_file, nrows=self.sample_size//2)
            print(f"âœ… Failure trace: {len(failure_data):,} kayÄ±t yÃ¼klendi")
            
        except Exception as e:
            print(f"âŒ Preprocessed veriler yÃ¼klenemedi: {str(e)}")
            return None, None
        
        return normal_data, failure_data
    
    def explore_tracebench_data(self, data_dir="data/HDFS_big.log"):
        """Tracebench klasÃ¶rÃ¼nÃ¼ keÅŸfet"""
        print("\nğŸš€ TRACEBENCH VERÄ°LERÄ° KEÅFEDÄ°LÄ°YOR")
        print("=" * 50)
        
        tracebench_path = Path(data_dir) / "tracebench"
        
        # Anomali tÃ¼rlerini kategorize et
        anomaly_categories = {
            'Data_corrupt': [],
            'Data_cut': [],
            'Data_loss': [],
            'Net_disconnect': [],
            'Net_slow': [],
            'Proc_kill': [],
            'Proc_suspend': [],
            'Sys_dead': [],
            'Sys_panic': [],
            'Complex': [],
            'Normal': []
        }
        
        # TÃ¼m klasÃ¶rleri tara
        for folder in tracebench_path.iterdir():
            if folder.is_dir():
                folder_name = folder.name
                
                if folder_name.startswith('AN_Data_corrupt'):
                    anomaly_categories['Data_corrupt'].append(folder_name)
                elif folder_name.startswith('AN_Data_cut'):
                    anomaly_categories['Data_cut'].append(folder_name)
                elif folder_name.startswith('AN_Data_loss'):
                    anomaly_categories['Data_loss'].append(folder_name)
                elif folder_name.startswith('AN_Net_disconnect'):
                    anomaly_categories['Net_disconnect'].append(folder_name)
                elif folder_name.startswith('AN_Net_slow'):
                    anomaly_categories['Net_slow'].append(folder_name)
                elif folder_name.startswith('AN_Proc_kill'):
                    anomaly_categories['Proc_kill'].append(folder_name)
                elif folder_name.startswith('AN_Proc_suspend'):
                    anomaly_categories['Proc_suspend'].append(folder_name)
                elif folder_name.startswith('AN_Sys_dead'):
                    anomaly_categories['Sys_dead'].append(folder_name)
                elif folder_name.startswith('AN_Sys_panic'):
                    anomaly_categories['Sys_panic'].append(folder_name)
                elif folder_name.startswith('COM_'):
                    anomaly_categories['Complex'].append(folder_name)
                elif folder_name.startswith('NM_'):
                    anomaly_categories['Normal'].append(folder_name)
        
        # Ä°statistikleri yazdÄ±r
        print("ğŸ“Š ANOMALÄ° TÃœRÃœ Ä°STATÄ°STÄ°KLERÄ°:")
        total_scenarios = 0
        for category, folders in anomaly_categories.items():
            count = len(folders)
            total_scenarios += count
            if count > 0:
                print(f"  {category.replace('_', ' ').title()}: {count:,} senaryo")
        
        print(f"\nToplam senaryo sayÄ±sÄ±: {total_scenarios:,}")
        
        return anomaly_categories
    
    def sample_diverse_scenarios(self, anomaly_categories, samples_per_category=2):
        """Her kategoriden Ã¶rnekler seÃ§"""
        print(f"\nğŸ¯ Her kategoriden {samples_per_category} Ã¶rnek seÃ§iliyor...")
        
        selected_scenarios = {}
        total_selected = 0
        
        for category, folders in anomaly_categories.items():
            if folders and len(folders) > 0:
                # Rastgele Ã¶rnekle
                n_samples = min(samples_per_category, len(folders))
                selected = np.random.choice(folders, n_samples, replace=False)
                selected_scenarios[category] = list(selected)
                total_selected += n_samples
                print(f"  {category}: {n_samples} senaryo seÃ§ildi")
        
        print(f"âœ… Toplam {total_selected} senaryo seÃ§ildi")
        return selected_scenarios
    
    def load_scenario_logs(self, scenario_path):
        """Bir senaryonun log dosyalarÄ±nÄ± yÃ¼kle"""
        logs = []
        
        try:
            scenario_dir = Path(scenario_path)
            
            # event.csv dosyasÄ±ndan log verileri al
            event_file = scenario_dir / "event.csv"
            if event_file.exists():
                try:
                    df = pd.read_csv(event_file, nrows=1000)  # Ä°lk 1000 kayÄ±t
                    if 'Description' in df.columns and 'OpName' in df.columns:
                        # OpName ve Description'Ä± birleÅŸtirerek log mesajÄ± oluÅŸtur
                        log_messages = []
                        for _, row in df.iterrows():
                            log_msg = f"{row['OpName']}: {row['Description']}"
                            log_messages.append(log_msg)
                        logs.extend(log_messages)
                except Exception as e:
                    print(f"âš ï¸ event.csv okuma hatasÄ±: {str(e)}")
            
            # trace.csv dosyasÄ±ndan da veri al
            trace_file = scenario_dir / "trace.csv"
            if trace_file.exists() and len(logs) < 500:  # Yeterli veri yoksa trace'den de al
                try:
                    df = pd.read_csv(trace_file, nrows=500)
                    if 'Title' in df.columns:
                        for _, row in df.iterrows():
                            log_msg = f"Task: {row['Title']} NumReports: {row['NumReports']}"
                            logs.append(log_msg)
                except Exception as e:
                    print(f"âš ï¸ trace.csv okuma hatasÄ±: {str(e)}")
                    
        except Exception as e:
            print(f"âš ï¸ {scenario_path} yÃ¼klenirken hata: {str(e)}")
        
        return logs
    
    def process_mega_dataset(self, data_dir="data/HDFS_big.log"):
        """Mega veri seti iÅŸleme ana fonksiyonu"""
        print("ğŸš€ MEGA HDFS TRACE VERÄ° SETÄ° Ä°ÅLEME BAÅLADI")
        print("=" * 70)
        
        start_time = time.time()
        
        # 1. Tracebench verilerini keÅŸfet
        anomaly_categories = self.explore_tracebench_data(data_dir)
        
        # 2. Ã‡eÅŸitli senaryolarÄ± Ã¶rnekle
        selected_scenarios = self.sample_diverse_scenarios(anomaly_categories, samples_per_category=3)
        
        # 3. SeÃ§ilen senaryolardan log verilerini topla
        print("\nğŸ“š SeÃ§ilen senaryolardan log verileri toplanÄ±yor...")
        
        all_logs = []
        scenario_labels = []
        
        tracebench_path = Path(data_dir) / "tracebench"
        
        for category, scenarios in selected_scenarios.items():
            for scenario in scenarios:
                scenario_path = tracebench_path / scenario
                
                print(f"  {scenario} iÅŸleniyor...")
                scenario_logs = self.load_scenario_logs(scenario_path)
                
                if scenario_logs:
                    # Max 1000 log per scenario
                    sample_size = min(1000, len(scenario_logs))
                    sampled_logs = np.random.choice(scenario_logs, sample_size, replace=False)
                    
                    all_logs.extend(sampled_logs)
                    scenario_labels.extend([category] * len(sampled_logs))
                    
                    print(f"    âœ… {len(sampled_logs)} log eklendi")
        
        print(f"\nğŸ“Š Toplam toplanan log: {len(all_logs):,}")
        print(f"ğŸ“Š Kategoriler: {len(set(scenario_labels))}")
        
        # 4. LogBERT ile anomali tespiti
        if len(all_logs) > 0:
            results = self.run_logbert_on_mega_data(all_logs, scenario_labels)
        else:
            print("âŒ HiÃ§ log verisi toplanamadÄ±!")
            return None
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"\nâ±ï¸ TOPLAM Ä°ÅLEM SÃœRESÄ°: {processing_time:.1f} saniye")
        print("=" * 70)
        print("âœ… MEGA VERÄ° SETÄ° Ä°ÅLEME TAMAMLANDI!")
        
        return results
    
    def run_logbert_on_mega_data(self, logs, labels):
        """Toplanan mega veri Ã¼zerinde LogBERT Ã§alÄ±ÅŸtÄ±r"""
        print("\nğŸ§  MEGA VERÄ° ÃœZERÄ°NDE LogBERT ANOMALÄ° TESPÄ°TÄ°")
        print("=" * 50)
        
        # Veri Ã¶n iÅŸleme
        processed_logs = self.preprocess_mega_logs(logs)
        
        # BERT embeddings Ã§Ä±karma
        embeddings = self.extract_embeddings_mega(processed_logs)
        
        # Anomali tespiti
        anomaly_results = self.detect_anomalies_mega(embeddings, labels)
        
        # SonuÃ§larÄ± analiz et ve gÃ¶rselleÅŸtir
        self.analyze_mega_results(anomaly_results, labels, processed_logs)
        
        return {
            'logs': processed_logs,
            'embeddings': embeddings,
            'anomaly_results': anomaly_results,
            'labels': labels
        }
    
    def preprocess_mega_logs(self, logs):
        """Mega veri log Ã¶n iÅŸleme"""
        print("ğŸ“ Mega veri log Ã¶n iÅŸleme...")
        
        import re
        processed = []
        
        for log in tqdm(logs[:self.sample_size], desc="Log preprocessing"):
            if isinstance(log, str):
                # Temel temizleme
                log = re.sub(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}', '[TIMESTAMP]', log)
                log = re.sub(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}(:\d+)?', '[IP]', log)
                log = re.sub(r'blk_-?\d+', '[BLOCK_ID]', log)
                log = re.sub(r'\b\d{5,}\b', '[NUMBER]', log)
                
                processed.append(log.strip())
        
        print(f"âœ… {len(processed)} log iÅŸlendi")
        return processed
    
    def extract_embeddings_mega(self, logs):
        """Mega veri iÃ§in BERT embeddings"""
        print("ğŸ§  Mega veri BERT embeddings...")
        
        all_embeddings = []
        
        with torch.no_grad():
            for i in tqdm(range(0, len(logs), self.batch_size), desc="BERT embedding"):
                batch = logs[i:i+self.batch_size]
                
                try:
                    encoded = self.tokenizer(
                        batch,
                        truncation=True,
                        padding=True,
                        max_length=self.max_length,
                        return_tensors='pt'
                    )
                    
                    input_ids = encoded['input_ids'].to(self.device)
                    attention_mask = encoded['attention_mask'].to(self.device)
                    
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                    
                    all_embeddings.append(embeddings)
                    
                    # Memory cleanup
                    del input_ids, attention_mask, outputs
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
                    
                except Exception as e:
                    continue
        
        final_embeddings = np.vstack(all_embeddings) if all_embeddings else np.array([])
        print(f"âœ… {final_embeddings.shape[0]} embedding Ã§Ä±karÄ±ldÄ±")
        
        return final_embeddings
    
    def detect_anomalies_mega(self, embeddings, labels):
        """Mega veri anomali tespiti"""
        print("ğŸ” Mega veri anomali tespiti...")
        
        if len(embeddings) == 0:
            return None
        
        # Normalize
        normalized = self.scaler.fit_transform(embeddings)
        
        # MiniBatch K-Means clustering
        n_clusters = max(5, min(20, len(set(labels))))
        kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)
        cluster_labels = kmeans.fit_predict(normalized)
        
        # Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomaly_scores = iso_forest.fit_predict(normalized)
        
        results = {
            'cluster_labels': cluster_labels,
            'anomaly_scores': anomaly_scores,
            'normalized_embeddings': normalized
        }
        
        anomaly_count = np.sum(anomaly_scores == -1)
        print(f"ğŸš¨ {anomaly_count} anomali tespit edildi ({anomaly_count/len(embeddings)*100:.1f}%)")
        
        return results
    
    def analyze_mega_results(self, results, labels, logs):
        """Mega veri sonuÃ§larÄ±nÄ± analiz et"""
        print("ğŸ“Š Mega veri sonuÃ§larÄ± analiz ediliyor...")
        
        if results is None:
            return
        
        # Kategori bazlÄ± anomali analizi
        category_anomalies = {}
        
        for i, (label, anomaly_score) in enumerate(zip(labels, results['anomaly_scores'])):
            if label not in category_anomalies:
                category_anomalies[label] = {'total': 0, 'anomalies': 0}
            
            category_anomalies[label]['total'] += 1
            if anomaly_score == -1:
                category_anomalies[label]['anomalies'] += 1
        
        # SonuÃ§larÄ± yazdÄ±r
        print("\nğŸ“ˆ KATEGORÄ° BAZLI ANOMALÄ° ANALÄ°ZÄ°:")
        for category, stats in category_anomalies.items():
            total = stats['total']
            anomalies = stats['anomalies']
            percentage = (anomalies / total * 100) if total > 0 else 0
            print(f"  {category}: {anomalies}/{total} ({percentage:.1f}%)")
        
        # GÃ¶rselleÅŸtirme
        self.visualize_mega_results(results, labels)
        
        # SonuÃ§larÄ± kaydet
        self.save_mega_results(results, labels, logs, category_anomalies)
    
    def visualize_mega_results(self, results, labels):
        """Mega veri sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtir"""
        print("ğŸ“Š Mega veri gÃ¶rselleÅŸtirmesi...")
        
        embeddings = results['normalized_embeddings']
        anomaly_scores = results['anomaly_scores']
        
        # PCA ile boyut indirgeme
        pca = IncrementalPCA(n_components=2)
        embeddings_2d = pca.fit_transform(embeddings)
        
        plt.figure(figsize=(15, 10))
        
        # Anomali gÃ¶rselleÅŸtirmesi
        plt.subplot(2, 2, 1)
        colors = ['blue' if score != -1 else 'red' for score in anomaly_scores]
        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors, alpha=0.6, s=20)
        plt.title('Mega Veri Anomali Tespiti')
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        
        # Kategori daÄŸÄ±lÄ±mÄ±
        plt.subplot(2, 2, 2)
        label_counts = pd.Series(labels).value_counts()
        plt.bar(range(len(label_counts)), label_counts.values)
        plt.title('Kategori DaÄŸÄ±lÄ±mÄ±')
        plt.xticks(range(len(label_counts)), label_counts.index, rotation=45)
        
        # Anomali oranlarÄ±
        plt.subplot(2, 2, 3)
        anomaly_by_category = {}
        for i, (label, score) in enumerate(zip(labels, anomaly_scores)):
            if label not in anomaly_by_category:
                anomaly_by_category[label] = []
            anomaly_by_category[label].append(1 if score == -1 else 0)
        
        categories = list(anomaly_by_category.keys())
        anomaly_rates = [np.mean(anomaly_by_category[cat]) * 100 for cat in categories]
        
        plt.bar(range(len(categories)), anomaly_rates)
        plt.title('Kategoriye GÃ¶re Anomali OranlarÄ± (%)')
        plt.xticks(range(len(categories)), categories, rotation=45)
        plt.ylabel('Anomali OranÄ± (%)')
        
        # Cluster daÄŸÄ±lÄ±mÄ±
        plt.subplot(2, 2, 4)
        cluster_labels = results['cluster_labels']
        unique_clusters = np.unique(cluster_labels)
        for cluster in unique_clusters:
            mask = cluster_labels == cluster
            plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], 
                       label=f'Cluster {cluster}', alpha=0.6, s=20)
        plt.title('Cluster DaÄŸÄ±lÄ±mÄ±')
        plt.xlabel('PC1')
        plt.ylabel('PC2')
        plt.legend()
        
        plt.tight_layout()
        plt.savefig('results/mega_data_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… Mega veri gÃ¶rselleÅŸtirmesi kaydedildi: results/mega_data_analysis.png")
    
    def save_mega_results(self, results, labels, logs, category_stats):
        """Mega veri sonuÃ§larÄ±nÄ± kaydet"""
        print("ğŸ’¾ Mega veri sonuÃ§larÄ± kaydediliyor...")
        
        # Ana raporu kaydet
        with open('results/mega_data_report.txt', 'w', encoding='utf-8') as f:
            f.write("ğŸš€ MEGA HDFS TRACE VERÄ° SETÄ° ANALÄ°Z RAPORU\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"Toplam iÅŸlenen log: {len(logs):,}\n")
            f.write(f"Toplam kategoriler: {len(set(labels))}\n")
            f.write(f"Tespit edilen anomaliler: {np.sum(results['anomaly_scores'] == -1):,}\n")
            f.write(f"Anomali oranÄ±: {np.sum(results['anomaly_scores'] == -1) / len(results['anomaly_scores']) * 100:.2f}%\n\n")
            
            f.write("ğŸ“Š KATEGORÄ° BAZLI Ä°STATÄ°STÄ°KLER:\n")
            f.write("-" * 40 + "\n")
            for category, stats in category_stats.items():
                percentage = (stats['anomalies'] / stats['total'] * 100) if stats['total'] > 0 else 0
                f.write(f"{category}: {stats['anomalies']}/{stats['total']} ({percentage:.1f}%)\n")
        
        print("âœ… Mega veri raporu kaydedildi: results/mega_data_report.txt")

def main():
    """Ana fonksiyon"""
    print("ğŸŒŸ MEGA HDFS TRACE VERÄ° ANALÄ°ZÄ° BAÅLADI")
    print("=" * 70)
    
    # Mega LogBERT sistemi oluÅŸtur
    mega_logbert = MegaDataLogBERT(
        max_length=64,    # Daha kÃ¼Ã§Ã¼k max length
        batch_size=4      # Daha kÃ¼Ã§Ã¼k batch size
    )
    
    # Mega veri setini iÅŸle
    results = mega_logbert.process_mega_dataset()
    
    return results

if __name__ == "__main__":
    results = main()

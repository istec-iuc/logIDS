import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer, AdamW
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import pickle
from pathlib import Path
warnings.filterwarnings('ignore')

class SupervisedLogBERT(nn.Module):
    """Etiketli anomali sÄ±nÄ±flandÄ±rmasÄ± iÃ§in LogBERT"""
    
    def __init__(self, model_name='bert-base-uncased', num_classes=11, hidden_size=768, dropout_rate=0.3):
        super(SupervisedLogBERT, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(hidden_size, num_classes)
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        cls_output = self.dropout(cls_output)
        logits = self.classifier(cls_output)
        return logits

class LogDataset(Dataset):
    """Log verileri iÃ§in PyTorch Dataset"""
    
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class SupervisedHDFSClassifier:
    """Supervised HDFS Log Anomali SÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±"""
    
    def __init__(self, model_name='bert-base-uncased', max_length=128, batch_size=16):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ðŸ”§ KullanÄ±lan cihaz: {self.device}")
        
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.max_length = max_length
        self.batch_size = batch_size
        
        # SÄ±nÄ±f etiketleri
        self.class_names = [
            'Normal',
            'Data_corrupt', 
            'Data_cut',
            'Data_loss',
            'Net_disconnect',
            'Net_slow', 
            'Proc_kill',
            'Proc_suspend',
            'Sys_dead',
            'Sys_panic',
            'Complex'
        ]
        
        self.label_to_id = {label: idx for idx, label in enumerate(self.class_names)}
        self.id_to_label = {idx: label for idx, label in enumerate(self.class_names)}
        
        self.model = SupervisedLogBERT(num_classes=len(self.class_names))
        self.model.to(self.device)
    
    def load_labeled_data_from_scenarios(self, data_dir="data/HDFS_big.log"):
        """Senaryolardan etiketli veri yÃ¼kle"""
        print("ðŸ“Š ETÄ°KETLÄ° VERÄ° SETÄ° OLUÅžTURULUYOR")
        print("=" * 50)
        
        tracebench_path = Path(data_dir) / "tracebench"
        
        all_texts = []
        all_labels = []
        
        # Her kategoriden veri topla
        category_counts = {}
        
        for folder in tqdm(tracebench_path.iterdir(), desc="KlasÃ¶rler taranÄ±yor"):
            if folder.is_dir():
                folder_name = folder.name
                
                # Kategori belirle
                category = self._determine_category(folder_name)
                if category is None:
                    continue
                
                # Event.csv dosyasÄ±ndan loglarÄ± yÃ¼kle
                event_file = folder / "event.csv"
                if event_file.exists():
                    try:
                        df = pd.read_csv(event_file, nrows=200)  # Her senaryodan 200 Ã¶rnek
                        if 'Description' in df.columns and 'OpName' in df.columns:
                            for _, row in df.iterrows():
                                log_text = f"{row['OpName']}: {row['Description']}"
                                all_texts.append(log_text)
                                all_labels.append(category)
                                
                                # SayaÃ§
                                if category not in category_counts:
                                    category_counts[category] = 0
                                category_counts[category] += 1
                    except Exception as e:
                        continue
        
        print("\nðŸ“ˆ TOPLANAN ETÄ°KETLÄ° VERÄ°:")
        for category, count in category_counts.items():
            print(f"  {category}: {count:,} Ã¶rnek")
        
        print(f"\nToplam etiketli Ã¶rnek: {len(all_texts):,}")
        
        return all_texts, all_labels
    
    def _determine_category(self, folder_name):
        """KlasÃ¶r ismine gÃ¶re kategori belirle"""
        if folder_name.startswith('NM_'):
            return 'Normal'
        elif 'corruptBlk' in folder_name or 'corruptMeta' in folder_name:
            return 'Data_corrupt'
        elif 'cutBlk' in folder_name or 'cutMeta' in folder_name:
            return 'Data_cut'
        elif 'lossBlk' in folder_name or 'lossMeta' in folder_name:
            return 'Data_loss'
        elif 'disconnectDN' in folder_name:
            return 'Net_disconnect'
        elif 'slowDN' in folder_name or 'slowHDFS' in folder_name:
            return 'Net_slow'
        elif 'killDN' in folder_name:
            return 'Proc_kill'
        elif 'suspendDN' in folder_name:
            return 'Proc_suspend'
        elif 'deadDN' in folder_name:
            return 'Sys_dead'
        elif 'panicDN' in folder_name:
            return 'Sys_panic'
        elif folder_name.startswith('COM_'):
            return 'Complex'
        else:
            return None
    
    def prepare_data(self, texts, labels):
        """Veriyi eÄŸitim iÃ§in hazÄ±rla"""
        print("\nðŸ”§ VERÄ° EÄžÄ°TÄ°M Ä°Ã‡Ä°N HAZIRLANIYOR...")
        
        # Etiketleri sayÄ±sal deÄŸerlere Ã§evir
        numeric_labels = [self.label_to_id[label] for label in labels]
        
        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(
            texts, numeric_labels, 
            test_size=0.2, 
            random_state=42, 
            stratify=numeric_labels
        )
        
        print(f"EÄŸitim seti: {len(X_train):,} Ã¶rnek")
        print(f"Test seti: {len(X_test):,} Ã¶rnek")
        
        # Dataset oluÅŸtur
        train_dataset = LogDataset(X_train, y_train, self.tokenizer, self.max_length)
        test_dataset = LogDataset(X_test, y_test, self.tokenizer, self.max_length)
        
        # DataLoader oluÅŸtur
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)
        
        return train_loader, test_loader, X_test, y_test
    
    def train_model(self, train_loader, test_loader, epochs=3, learning_rate=2e-5):
        """Modeli eÄŸit"""
        print(f"\nðŸš€ MODEL EÄžÄ°TÄ°MÄ° BAÅžLADI ({epochs} epoch)")
        print("=" * 50)
        
        optimizer = AdamW(self.model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # Training loop
        for epoch in range(epochs):
            print(f"\nðŸ“š Epoch {epoch + 1}/{epochs}")
            
            # Training
            self.model.train()
            total_loss = 0
            correct_predictions = 0
            total_predictions = 0
            
            progress_bar = tqdm(train_loader, desc="EÄŸitim")
            for batch in progress_bar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                optimizer.zero_grad()
                
                outputs = self.model(input_ids, attention_mask)
                loss = criterion(outputs, labels)
                
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                predictions = torch.argmax(outputs, dim=-1)
                correct_predictions += (predictions == labels).sum().item()
                total_predictions += labels.size(0)
                
                # Progress bar gÃ¼ncelle
                accuracy = correct_predictions / total_predictions * 100
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Acc': f'{accuracy:.1f}%'
                })
            
            # Epoch sonuÃ§larÄ±
            epoch_loss = total_loss / len(train_loader)
            epoch_accuracy = correct_predictions / total_predictions * 100
            
            print(f"Epoch {epoch + 1} SonuÃ§larÄ±:")
            print(f"  Ortalama Loss: {epoch_loss:.4f}")
            print(f"  EÄŸitim DoÄŸruluÄŸu: {epoch_accuracy:.2f}%")
            
            # Validation
            val_accuracy = self.evaluate_model(test_loader, verbose=False)
            print(f"  Validasyon DoÄŸruluÄŸu: {val_accuracy:.2f}%")
        
        print("\nâœ… MODEL EÄžÄ°TÄ°MÄ° TAMAMLANDI!")
    
    def evaluate_model(self, test_loader, verbose=True):
        """Modeli deÄŸerlendir"""
        if verbose:
            print("\nðŸ“Š MODEL DEÄžERLENDÄ°RÄ°LÄ°YOR...")
        
        self.model.eval()
        correct_predictions = 0
        total_predictions = 0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids, attention_mask)
                predictions = torch.argmax(outputs, dim=-1)
                
                correct_predictions += (predictions == labels).sum().item()
                total_predictions += labels.size(0)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        accuracy = correct_predictions / total_predictions * 100
        
        if verbose:
            print(f"âœ… Test DoÄŸruluÄŸu: {accuracy:.2f}%")
            
            # SÄ±nÄ±flandÄ±rma raporu
            class_names = [self.id_to_label[i] for i in range(len(self.class_names))]
            report = classification_report(all_labels, all_predictions, target_names=class_names)
            print(f"\nðŸ“‹ SINIFLANDIRMA RAPORU:\n{report}")
            
            # Confusion matrix
            self.plot_confusion_matrix(all_labels, all_predictions)
        
        return accuracy
    
    def plot_confusion_matrix(self, true_labels, pred_labels):
        """Confusion matrix Ã§iz"""
        cm = confusion_matrix(true_labels, pred_labels)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=self.class_names,
                    yticklabels=self.class_names)
        plt.title('Confusion Matrix - Supervised LogBERT')
        plt.xlabel('Tahmin Edilen')
        plt.ylabel('GerÃ§ek')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig('results/supervised_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… Confusion matrix kaydedildi: results/supervised_confusion_matrix.png")
    
    def predict_single_log(self, text):
        """Tek bir log iÃ§in tahmin yap"""
        self.model.eval()
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        
        with torch.no_grad():
            outputs = self.model(input_ids, attention_mask)
            probabilities = torch.softmax(outputs, dim=-1)
            prediction = torch.argmax(outputs, dim=-1)
        
        predicted_class = self.id_to_label[prediction.item()]
        confidence = probabilities[0][prediction].item()
        
        return predicted_class, confidence
    
    def save_model(self, path='models/supervised_logbert.pth'):
        """Modeli kaydet"""
        Path(path).parent.mkdir(exist_ok=True)
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'class_names': self.class_names,
            'label_to_id': self.label_to_id,
            'id_to_label': self.id_to_label
        }, path)
        print(f"âœ… Model kaydedildi: {path}")
    
    def run_supervised_training(self):
        """Tam supervised eÄŸitim sÃ¼reci"""
        print("ðŸŽ¯ SUPERVISED HDFS LOG SINIFLANDIRICI")
        print("=" * 60)
        
        # 1. Etiketli veri yÃ¼kle
        texts, labels = self.load_labeled_data_from_scenarios()
        
        if len(texts) < 100:
            print("âŒ Yeterli veri bulunamadÄ±!")
            return None
        
        # 2. Veriyi hazÄ±rla
        train_loader, test_loader, X_test, y_test = self.prepare_data(texts, labels)
        
        # 3. Modeli eÄŸit
        self.train_model(train_loader, test_loader, epochs=3)
        
        # 4. Modeli deÄŸerlendir
        self.evaluate_model(test_loader)
        
        # 5. Modeli kaydet
        self.save_model()
        
        # 6. Ã–rnek tahminler
        print("\nðŸ§ª Ã–RNEK TAHMÄ°NLER:")
        test_examples = [
            "getFileInfo: Success: return file status",
            "BLOCK* ask datanode to delete corrupted block", 
            "RPC:getFileInfo connection timeout",
            "PacketResponder terminating due to network error"
        ]
        
        for example in test_examples:
            predicted_class, confidence = self.predict_single_log(example)
            print(f"Log: {example[:50]}...")
            print(f"Tahmin: {predicted_class} (GÃ¼ven: {confidence:.3f})")
            print("-" * 50)
        
        print("\nâœ… SUPERVISED SÄ°STEM HAZIR!")
        return self.model

def main():
    """Ana fonksiyon"""
    classifier = SupervisedHDFSClassifier(batch_size=8)  # KÃ¼Ã§Ã¼k batch size
    model = classifier.run_supervised_training()
    return classifier

if __name__ == "__main__":
    classifier = main()
